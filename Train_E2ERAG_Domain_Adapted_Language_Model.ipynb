{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This is a tutorial from Arcee and AI Makerspace on how to train an end to end RAG model jointly with retriever and generator to create a Domain Adapted Language Model (a DALM). E2E RAG training allows you to create performant contextualized retrievers for your RAG system.\n",
        "\n",
        "The model we train will learn how to retrieve documents from our context documents and generate responses in a unified system.\n",
        "\n",
        "We will:\n",
        "\n",
        "* Clone and Install DALM dependencies\n",
        "* Prepare a toy dataset\n",
        "* Run E2E RAG training for our DALM\n",
        "* Run inference on our trained DALM\n",
        "* Export our trained retriever model\n",
        "* Query example DALMs at https://app.arcee.ai\n",
        "\n",
        "![e2erag](https://i.imgur.com/0uMWN8H.png)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OIrkM5g_zyo0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwYNb2ZMzhoT",
        "outputId": "9ace3555-d7e1-4664-b417-23283774679d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Sep 23 09:36:01 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "#Make sure you have a GPU enabled\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clone DALM repo and Install dependenciese\n",
        "\n",
        "We will clone and install the dependencies in https://github.com/arcee-ai/DALM to prepare for our E2E RAG training\n",
        "\n",
        "**DALM**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lJHYB80fAHZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/arcee-ai/DALM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iluTPAeNAGev",
        "outputId": "12f213fa-def0-4f04-dcde-2ff37157c14d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DALM'...\n",
            "remote: Enumerating objects: 1366, done.\u001b[K\n",
            "remote: Counting objects: 100% (253/253), done.\u001b[K\n",
            "remote: Compressing objects: 100% (93/93), done.\u001b[K\n",
            "remote: Total 1366 (delta 192), reused 180 (delta 160), pack-reused 1113\u001b[K\n",
            "Receiving objects: 100% (1366/1366), 18.82 MiB | 22.33 MiB/s, done.\n",
            "Resolving deltas: 100% (782/782), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd DALM\n",
        "!pip install --upgrade -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u880kpH_zm11",
        "outputId": "394202ec-80be-4846-e061-87d346a8ac66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DALM\n",
            "Obtaining file:///content/DALM\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting accelerate (from indomain==0.0.4)\n",
            "  Downloading accelerate-0.23.0-py3-none-any.whl (258 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes (from indomain==0.0.4)\n",
            "  Downloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets (from indomain==0.0.4)\n",
            "  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diffusers (from indomain==0.0.4)\n",
            "  Downloading diffusers-0.21.2.tar.gz (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting evaluate (from indomain==0.0.4)\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hnswlib (from indomain==0.0.4)\n",
            "  Downloading hnswlib-0.7.0.tar.gz (33 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting peft (from indomain==0.0.4)\n",
            "  Downloading peft-0.5.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.6/85.6 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from indomain==0.0.4) (1.10.12)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from indomain==0.0.4) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from indomain==0.0.4) (1.11.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from indomain==0.0.4) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from indomain==0.0.4) (0.15.2+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from indomain==0.0.4) (4.66.1)\n",
            "Collecting transformers (from indomain==0.0.4)\n",
            "  Downloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m114.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer in /usr/local/lib/python3.10/dist-packages (from indomain==0.0.4) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate->indomain==0.0.4) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate->indomain==0.0.4) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->indomain==0.0.4) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate->indomain==0.0.4) (6.0.1)\n",
            "Collecting huggingface-hub (from accelerate->indomain==0.0.4)\n",
            "  Downloading huggingface_hub-0.17.2-py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->indomain==0.0.4) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->indomain==0.0.4) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->indomain==0.0.4) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->indomain==0.0.4) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->indomain==0.0.4) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->indomain==0.0.4) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->indomain==0.0.4) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->indomain==0.0.4) (16.0.6)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->indomain==0.0.4) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets->indomain==0.0.4)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->indomain==0.0.4) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets->indomain==0.0.4) (2.31.0)\n",
            "Collecting xxhash (from datasets->indomain==0.0.4)\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets->indomain==0.0.4)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets->indomain==0.0.4) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->indomain==0.0.4) (3.8.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers->indomain==0.0.4) (6.8.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers->indomain==0.0.4) (2023.6.3)\n",
            "Collecting safetensors>=0.3.1 (from diffusers->indomain==0.0.4)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers->indomain==0.0.4) (9.4.0)\n",
            "Collecting responses<0.19 (from evaluate->indomain==0.0.4)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->indomain==0.0.4) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->indomain==0.0.4) (3.2.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->indomain==0.0.4)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer->indomain==0.0.4) (8.1.7)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->indomain==0.0.4) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->indomain==0.0.4) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->indomain==0.0.4) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->indomain==0.0.4) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->indomain==0.0.4) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->indomain==0.0.4) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->indomain==0.0.4) (1.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->indomain==0.0.4) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->indomain==0.0.4) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->indomain==0.0.4) (2023.7.22)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers->indomain==0.0.4) (3.16.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->indomain==0.0.4) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->indomain==0.0.4) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->indomain==0.0.4) (2023.3.post1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->indomain==0.0.4) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->indomain==0.0.4) (1.16.0)\n",
            "Building wheels for collected packages: indomain, diffusers, hnswlib\n",
            "  Building editable for indomain (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for indomain: filename=indomain-0.0.4-py3-none-any.whl size=8350 sha256=07242b6a28c4b5dc29bb8a2d9a0014e6e2ac67b186f9e201158b1d4617c72ad6\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-yeb8mtou/wheels/6d/43/1f/e7ddd91e94374cfd2c1af120bda61c32241f5409ac222016cc\n",
            "  Building wheel for diffusers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for diffusers: filename=diffusers-0.21.2-py3-none-any.whl size=1489250 sha256=b3836a0445fe4f5d6e05d875ddbcac4144fb112f22ccd70a38100f530537d2c0\n",
            "  Stored in directory: /root/.cache/pip/wheels/2e/09/32/11c9e42c397d3f3494226b28ba68c4ad4718a68a65dba14ea6\n",
            "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hnswlib: filename=hnswlib-0.7.0-cp310-cp310-linux_x86_64.whl size=2202688 sha256=edd75aea5e88bdb79ff57ab507db3b2f1245c89d05d24ef78fddf40dd00c8dd7\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/ae/ec/235a682e0041fbaeee389843670581ec6c66872db856dfa9a4\n",
            "Successfully built indomain diffusers hnswlib\n",
            "Installing collected packages: tokenizers, safetensors, bitsandbytes, xxhash, hnswlib, dill, responses, multiprocess, huggingface-hub, transformers, diffusers, datasets, evaluate, accelerate, peft, indomain\n",
            "Successfully installed accelerate-0.23.0 bitsandbytes-0.41.1 datasets-2.14.5 diffusers-0.21.2 dill-0.3.7 evaluate-0.4.0 hnswlib-0.7.0 huggingface-hub-0.17.2 indomain-0.0.4 multiprocess-0.70.15 peft-0.5.0 responses-0.18.0 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.2 xxhash-3.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Dataset of Examples\n",
        "\n",
        "E2E RAG requires a dataset of [passage, question, answer] triples\n",
        "\n",
        "At inference time, our model will take a users query, draw from the available passages, and pass relevant context to the generator to create an answer.\n",
        "\n",
        "## Toy Dataset\n",
        "\n",
        "For this tutorial we will train a simple toy dataset to show how the E2E RAG training can be accomplished. To build your own DALM, you should train on your own documents\n",
        "\n",
        "## Generating Q+A Pairs\n",
        "\n",
        "If you do not have labeled triples, you can generate QA pairs with:\n",
        "\n",
        "```\n",
        "dalm qa-gen dalm/datasets/toy_data_train.csv\n",
        "```\n",
        "\n",
        "for example to generate QA pairs with the \"taesiri/arxiv_qa\" hugging face dataset:\n",
        "\n",
        "```\n",
        "!dalm qa-gen \"taesiri/arxiv_qa\" --output-dir qa-outputs --passage-column-name text --title-column-name title\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "lmtQZjf1EVIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#let's take a look at our toy_data\n",
        "%cat ./dalm/datasets/toy_data_train.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfXjtSZEEGrK",
        "outputId": "98be252b-c681-47bd-8eea-6772f8e8b28d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question,Abstract,Answer\n",
            "Photosynthesis definition,Process where plants convert light into energy through chlorophyll.,Energy conversion in plants\n",
            "Author of \"Romeo and Juliet\",Famous play written by William Shakespeare.,William Shakespeare\n",
            "Capital of France,The capital city of France is Paris.,Paris\n",
            "Declaration of Independence date,\"Document signed on July 4, 1776 declaring American independence.,July 4\", 1776\n",
            "World's largest ocean,The Pacific Ocean is the largest and deepest ocean on Earth.,Pacific Ocean\n",
            "Inventor of the telephone,Alexander Graham Bell invented the first practical telephone.,Alexander Graham Bell\n",
            "Natural satellite of Earth,The Moon is Earth's only natural satellite.,Moon\n",
            "Element with symbol \"H\",Hydrogen is a chemical element with the symbol H.,Hydrogen\n",
            "Novel \"Pride and Prejudice\" \"author,Classic novel written by Jane Austen.\",Jane Austen\n",
            "Human body's powerhouse,Mitochondria are the powerhouse of the cell.,Mitochondria\n",
            "Light's speed,\"The speed of light in a vacuum is about 299,792 kilometers per second.\",\"299,792 km/s\"\n",
            "Planet closest to the Sun,Mercury is the closest planet to the Sun in our solar system.,Mercury\n",
            "Largest mammal on Earth,The blue whale is the largest mammal to have ever existed.,Blue whale\n",
            "Element with symbol \"O\",Oxygen is a chemical element with the symbol O.,Oxygen\n",
            "Unit of electrical resistance,The ohm is the SI unit of electrical resistance.,Ohm\n",
            "The \"Iliad\" and the \"Odyssey\" attributed to,\"Homer, ancient Greek epic poet.\",Homer\n",
            "Process converting sugar to energy,Glycolysis is the initial process of cellular respiration.,Glycolysis\n",
            "Smallest prime number,The smallest prime number is 2.,2\n",
            "Number of continents on Earth,There are 7 continents on Earth.,7"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train DALM model with gpt-neo-125m generator and bge-large-en retriever\n",
        "\n",
        "For this demo notebook, we will train a small 125 million GPT Neo model and we will train the small bge retriever model.\n",
        "\n",
        "For better results, you will want to substitute `meta-llama/Llama-2-7b` and `bge-large-en`. You will need a bigger GPU (like A10080GB) to train this - and make sure to adjust batch size to saturate your GPU memory\n"
      ],
      "metadata": {
        "id": "V6gg1aeVKw--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!dalm train-rag-e2e \\\n",
        "\"./dalm/datasets/toy_data_train.csv\" \\\n",
        "\"BAAI/bge-large-en\" \\\n",
        "\"EleutherAI/gpt-neo-125m\" \\\n",
        "--output-dir \"./dalm/training/rag_e2e/rag_e2e_checkpoints\" \\\n",
        "--with-tracking \\\n",
        "--report-to all \\\n",
        "--per-device-train-batch-size 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yq9rtzyxKrPb",
        "outputId": "aacab041-96ae-47a8-e7b0-d5390d1e34fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-09-23 09:37:20.512047: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Downloading (…)lve/main/config.json: 100% 720/720 [00:00<00:00, 3.64MB/s]\n",
            "Downloading model.safetensors: 100% 1.34G/1.34G [00:05<00:00, 266MB/s]\n",
            "Downloading (…)okenizer_config.json: 100% 366/366 [00:00<00:00, 2.44MB/s]\n",
            "Downloading (…)solve/main/vocab.txt: 100% 232k/232k [00:00<00:00, 3.69MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 711k/711k [00:00<00:00, 10.4MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 125/125 [00:00<00:00, 772kB/s]\n",
            "Downloading (…)lve/main/config.json: 100% 1.01k/1.01k [00:00<00:00, 7.07MB/s]\n",
            "Downloading model.safetensors: 100% 526M/526M [00:01<00:00, 291MB/s]\n",
            "Downloading (…)okenizer_config.json: 100% 560/560 [00:00<00:00, 3.20MB/s]\n",
            "Downloading (…)olve/main/vocab.json: 100% 899k/899k [00:00<00:00, 6.96MB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100% 456k/456k [00:00<00:00, 6.87MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 357/357 [00:00<00:00, 2.43MB/s]\n",
            "09/23/2023 09:37:46 - INFO - dalm.training.rag_e2e.train_rage2e - Distributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "\n",
            "Mixed precision type: no\n",
            "\n",
            "Downloading data files: 100% 1/1 [00:00<00:00, 10106.76it/s]\n",
            "Extracting data files: 100% 1/1 [00:00<00:00, 311.59it/s]\n",
            "Generating train split: 19 examples [00:00, 666.88 examples/s]\n",
            "Running tokenizer on dataset: 100% 19/19 [00:00<00:00, 339.60 examples/s]\n",
            "09/23/2023 09:37:53 - INFO - dalm.training.rag_e2e.train_rage2e - Sample 3 of the training set: {'retriever_query_input_ids': [101, 1001, 23032, 1001, 8170, 1997, 4336, 3058, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'retriever_query_token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'retriever_query_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'retriever_passage_input_ids': [101, 1001, 6019, 1001, 6254, 2772, 2006, 2251, 1018, 1010, 13963, 13752, 2137, 4336, 1012, 1010, 2251, 1018, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'retriever_passage_token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'retriever_passage_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'generator_input_input_ids': [2, 22766, 2, 1303, 22766, 2, 24720, 286, 20153, 3128, 1303, 6603, 496, 2, 1303, 6603, 496, 2, 16854, 4488, 319, 2901, 604, 11, 1596, 4304, 18684, 1605, 10404, 1539, 16157, 604, 1303, 41484, 2, 220, 1596, 4304, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'generator_input_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'query_passage_input_len': 35}.\n",
            "09/23/2023 09:37:53 - INFO - dalm.training.rag_e2e.train_rage2e - Sample 0 of the training set: {'retriever_query_input_ids': [101, 1001, 23032, 1001, 7760, 6038, 25078, 6210, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'retriever_query_token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'retriever_query_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'retriever_passage_input_ids': [101, 1001, 6019, 1001, 2832, 2073, 4264, 10463, 2422, 2046, 2943, 2083, 10381, 10626, 7361, 29598, 2140, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'retriever_passage_token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'retriever_passage_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'generator_input_input_ids': [2, 22766, 2, 1303, 22766, 2, 9434, 44411, 6770, 1303, 6603, 496, 2, 1303, 6603, 496, 2, 10854, 810, 6134, 10385, 1657, 656, 2568, 832, 20207, 16982, 297, 13, 1303, 41484, 2, 6682, 11315, 287, 6134, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'generator_input_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'query_passage_input_len': 32}.\n",
            "09/23/2023 09:37:53 - INFO - dalm.training.rag_e2e.train_rage2e - ***** Running training *****\n",
            "09/23/2023 09:37:53 - INFO - dalm.training.rag_e2e.train_rage2e -   Num examples = 19\n",
            "09/23/2023 09:37:53 - INFO - dalm.training.rag_e2e.train_rage2e -   Num Epochs = 1\n",
            "09/23/2023 09:37:53 - INFO - dalm.training.rag_e2e.train_rage2e -   Instantaneous batch size per device = 1\n",
            "09/23/2023 09:37:53 - INFO - dalm.training.rag_e2e.train_rage2e -   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "09/23/2023 09:37:53 - INFO - dalm.training.rag_e2e.train_rage2e -   Gradient Accumulation steps = 1\n",
            "09/23/2023 09:37:53 - INFO - dalm.training.rag_e2e.train_rage2e -   Total optimization steps = 19\n",
            "100% 19/19 [00:10<00:00,  2.52it/s]epoch 0: {}\n",
            "tokenizer config file saved in ./dalm/training/rag_e2e/rag_e2e_checkpoints/retriever/tokenizer_config.json\n",
            "Special tokens file saved in ./dalm/training/rag_e2e/rag_e2e_checkpoints/retriever/special_tokens_map.json\n",
            "tokenizer config file saved in ./dalm/training/rag_e2e/rag_e2e_checkpoints/generator/tokenizer_config.json\n",
            "Special tokens file saved in ./dalm/training/rag_e2e/rag_e2e_checkpoints/generator/special_tokens_map.json\n",
            "100% 19/19 [00:10<00:00,  1.81it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate Retriever Training\n",
        "\n",
        "Here we will evalauate our contextualized retriever against our passage data that we will have in our database at retrieval time"
      ],
      "metadata": {
        "id": "9KxrPiw3Mwaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#must login to HF for this eval\n",
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GU-m75KCPJ00",
        "outputId": "eb61a236-8f3d-487f-e776-110ed66c7d69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "    \n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python dalm/eval/eval_rag.py \\\n",
        "  --dataset_path \"./dalm/datasets/toy_data_train.csv\" \\\n",
        "  --retriever_name_or_path \"BAAI/bge-large-en\" \\\n",
        "  --generator_name_or_path \"EleutherAI/gpt-neo-125m\" \\\n",
        "  --passage_column_name Abstract \\\n",
        "  --query_column_name Question \\\n",
        "  --answer_column_name Answer \\\n",
        "  --evaluate_generator \\\n",
        "  --query_batch_size 5 \\\n",
        "  --retriever_peft_model_path ./dalm/training/rag_e2e/rag_e2e_checkpoints/retriever \\\n",
        "  --generator_peft_model_path ./dalm/training/rag_e2e/rag_e2e_checkpoints/generator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8ttkEwbMvcc",
        "outputId": "f7834096-4796-4223-fc44-97d212a6779e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-09-23 09:40:26.761241: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Filter: 100% 19/19 [00:00<00:00, 1220.08 examples/s]\n",
            "Starting to generate passage embeddings (Number of passages: 19)\n",
            "100% 3/3 [00:00<00:00, 14.76it/s]\n",
            "Construct passage index\n",
            "Evaluation start\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Retriever results:\n",
            "Recall: 1.0\n",
            "Precision: 0.10000000000000003\n",
            "Hit Rate: 1.0\n",
            "*************\n",
            "Generator evaluation:\n",
            "Exact match: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query Example DALM\n",
        "\n",
        "In this tutorial, we showed how to train a retriever model with a small generator. To see a DALM in production, we can query DALM models on https://app.arcee.ai to get an idea for their behavior at scale.\n",
        "\n",
        "For example, we can query `DALM-Patent` on https://app.arcee.ai that has been trained on the USPTO Patent database\n",
        "\n",
        "![infergif](https://arcee-public.s3.us-east-2.amazonaws.com/infer.gif)\n"
      ],
      "metadata": {
        "id": "JCs5cKxuPvqo"
      }
    }
  ]
}